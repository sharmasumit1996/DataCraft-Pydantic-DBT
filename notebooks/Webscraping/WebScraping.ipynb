{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "from pydantic import HttpUrl\n",
    "import time, csv, sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_content(driver,page_link: HttpUrl) :\n",
    "    \"\"\"Function to scrape content using BeautifulSoup\"\"\"\n",
    "    link1 = page_link\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    title_class = soup.find('h1', class_='article-title')\n",
    "    \n",
    "    # Extract the article title text\n",
    "    title = title_class.text.strip()\n",
    "    \n",
    "    # Extract the topic\n",
    "    topic_span = soup.find('span', class_='content-utility-topics')\n",
    "    if topic_span is not None:\n",
    "        topic_text = topic_span.get_text(strip=True)\n",
    "    else:\n",
    "        topic_text = \"Doesn't Exist\"\n",
    "\n",
    "\n",
    "    # Extract the year text\n",
    "    year_span = soup.find('span', class_='content-utility-curriculum')\n",
    "    if year_span is not None:\n",
    "        year_text = year_span.get_text(strip=True)\n",
    "    else:\n",
    "        year_text = \"9999\"\n",
    "\n",
    "    # Extract the level text\n",
    "    level_span = soup.find('span', class_='content-utility-level')\n",
    "    # Find the span with class \"content-utility-topic\" within the level_span\n",
    "    if level_span is not None:\n",
    "        level_text = level_span.find('span', class_='content-utility-topic').text.strip()\n",
    "    else:\n",
    "        level_text = \"Level XX\"\n",
    "\n",
    "    # Find the introduction paragraphs\n",
    "    introduction_section = soup.find('h2', class_='article-section', string=['Introduction', 'Overview'])\n",
    "\n",
    "    # Find all paragraphs within the Introduction section\n",
    "    if introduction_section is not None:\n",
    "        intro_paragraphs = introduction_section.find_next_siblings('p')\n",
    "        # Extract the text from the paragraphs\n",
    "        paragraphs = ''\n",
    "        for p in intro_paragraphs:\n",
    "            # Check if the paragraph is within the Example section\n",
    "            if p.find_parents('figure', class_='example'):\n",
    "                break\n",
    "            # Append text from paragraph\n",
    "            paragraphs += p.get_text(strip=True) + ' '\n",
    "    else: \n",
    "        paragraphs = \"Doesn't Exist\"\n",
    "\n",
    "    # Find all <li> elements within the <ol> element to extract Learning Outcomes text\n",
    "    learning_outcomes_section = soup.find('h2', class_='article-section', string='Learning Outcomes')\n",
    "    if learning_outcomes_section is not None:\n",
    "        outcomes_section= learning_outcomes_section.find_next_sibling()\n",
    "        bullet_points = [li.get_text(strip=True) for li in outcomes_section.find_all(['li'])] \n",
    "        if bullet_points is None:\n",
    "            bullet_points = [li.get_text(strip=True) for li in outcomes_section.find_all(['p'])]\n",
    "        \n",
    "        bullet = '\\n'.join(bullet_points)\n",
    "    else:\n",
    "        bullet=\"Doesn't Exist\"\n",
    "\n",
    "    # Find the <a> tag for Full PDF link\n",
    "    # get the content from the PDF\n",
    "    locked_content_links = soup.find_all('a', class_='locked-content')\n",
    "\n",
    "    # get rid of the underlined-anchor\n",
    "    target_links = [link for link in locked_content_links if 'underlined-anchor' not in link.get('class', [])]\n",
    "\n",
    "    full_link = \"www.example.org\" # init\n",
    "    for link_tag in target_links:\n",
    "        if link_tag is not None:\n",
    "            link = link_tag['href']\n",
    "            full_link = \"https://www.cfainstitute.org\" + link\n",
    "        \n",
    "        \n",
    "    # Store all extracted data in list format\n",
    "    data = [title,topic_text, year_text, level_text, paragraphs, bullet, full_link, link1]\n",
    "    \n",
    "    return data\n",
    "    # print(*data)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_coveo_link(driver, link):\n",
    "    \"\"\"Function to click CoveoLink and return to main page\"\"\"\n",
    "    driver.execute_script(\"window.open('{}', '_blank');\".format(link))\n",
    "    \n",
    "    # Switch to new tab if opened\n",
    "    if len(driver.window_handles) > 1:\n",
    "        driver.switch_to.window(driver.window_handles[1])\n",
    "    \n",
    "    # Scraping content\n",
    "    scrape_data = scrape_content(driver, link)\n",
    "    \n",
    "    # Closing the tab and switching back to main page\n",
    "    driver.close()\n",
    "    driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "    return scrape_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def webScrabing():\n",
    "    \"\"\"Function to return the articles list\"\"\"\n",
    "    articles = []\n",
    "    \n",
    "    driver = webdriver.Chrome()  # Optional argument, if not specified will search path.\n",
    "    time.sleep(2)\n",
    "    \n",
    "    for offset in [0, 100, 200]:\n",
    "        main_frame = f'https://www.cfainstitute.org/membership/professional-development/refresher-readings#first={offset}&sort=@refreadingcurriculumyeardescending&numberOfResults=100'\n",
    "        driver.get(main_frame)\n",
    "        time.sleep(2)\n",
    "    \n",
    "        # Wait for CoveoLinks to load\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"CoveoResultLink\")))\n",
    "        \n",
    "        # Find all CoveoLinks\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        links = soup.find_all('a', class_='CoveoResultLink')\n",
    "    \n",
    "        # Extract the 'href' attribute from each link\n",
    "        coveo_links = [link['href'] for link in links]\n",
    "        print(len(links))     # To check no. of links extracted from the page\n",
    "        for link in coveo_links:\n",
    "            try:\n",
    "                articles.append( process_coveo_link(driver, link) )    \n",
    "                # Wait for some time to simulate human-like behavior\n",
    "                time.sleep(0.5)\n",
    "            except Exception as e:\n",
    "                print(f\"Error when get data: {e}\")\n",
    "                driver.close()\n",
    "                driver.switch_to.window(driver.window_handles[0])\n",
    "                \n",
    "    driver.quit()\n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## print the log\n",
    "log_file = open(\"../../data/log.txt\", \"w\")\n",
    "original_stdout = sys.stdout\n",
    "sys.stdout = log_file\n",
    "\n",
    "\n",
    "articles = webScrabing()\n",
    "\n",
    "sys.stdout = original_stdout\n",
    "log_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224 Succeed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# point the file position\n",
    "csv_file = \"../../data/items.csv\"\n",
    "\n",
    "\n",
    "with open(csv_file, 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(['NameOfTopic','Title','Year','Level','Introduction','LearningOutcome','LinkToPDF','LinkToSummary'])\n",
    "    for article in articles:\n",
    "        csvwriter.writerow(article)\n",
    "\n",
    "\n",
    "print(len(articles), 'Succeed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
